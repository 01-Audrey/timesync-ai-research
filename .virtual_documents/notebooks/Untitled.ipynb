"""
==================================================
TIMESYNC AI RESEARCH - SMART TIMESHEET PRE-FILL
==================================================
Project: OrasSync 2.0 - Smart Timesheet Pre-fill Feature
Sprint: ML Research Phase
Current Week: 1
Date: February 16, 2026
Notebook: Feature Engineering

ðŸ“Š PROJECT PHASES:
âœ… Phase 1: Data Exploration (COMPLETED)
ðŸ”„ Phase 2: Feature Engineering (IN PROGRESS)
â¬œ Phase 3: Random Forest Model Training
â¬œ Phase 4: Model Evaluation
â¬œ Phase 5: Production Integration

ðŸŽ¯ PHASE 2 DELIVERABLES:
ðŸ”„ Feature extraction (THIS NOTEBOOK)
â¬œ Feature encoding & normalization
â¬œ Training/test dataset creation
â¬œ Feature importance analysis

Progress: 25% (1/4 tasks)

ðŸŽ¯ THIS NOTEBOOK'S OBJECTIVES:
1. Extract ML-ready features from timesheet data
2. Engineer pattern-based features (frequency, consistency)
3. Create temporal features (day-of-week encoding)
4. Build feature vectors for Random Forest training
5. Save processed dataset for model training

ðŸ“š NOTEBOOK STRUCTURE:
Part 1: Load Data & Feature Planning
Part 2: Temporal Feature Engineering
Part 3: Pattern Feature Engineering
Part 4: Feature Encoding & Normalization
Part 5: Dataset Creation & Export

âœ… SUCCESS CRITERIA:
âœ… Extract all required ML features
âœ… Encode categorical variables properly
âœ… Create balanced training dataset
âœ… Save processed features for modeling
âœ… Document feature definitions

==================================================
"""


# ==================================================
# INSTALL REQUIRED LIBRARIES
# ==================================================
import sys
get_ipython().getoutput("{sys.executable} -m pip install pandas numpy matplotlib seaborn scikit-learn -q")
print("âœ… Libraries installed!")
print("\n" + "=" * 80)

# ==================================================
# IMPORT LIBRARIES
# ==================================================
print("\n" + "=" * 80)
print("ðŸ“š IMPORTING LIBRARIES")
print("=" * 80)

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)

# Set plot style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("\nâœ… All libraries imported successfully!")
print("=" * 80)

# ==================================================
# CREATE OUTPUT DIRECTORIES
# ==================================================
print("\n" + "=" * 80)
print("ðŸ“ CREATING OUTPUT DIRECTORIES")
print("=" * 80)

os.makedirs('../data/processed', exist_ok=True)
os.makedirs('../results/features', exist_ok=True)

print("\nâœ… Directories created:")
print("   - data/processed/ (for engineered features)")
print("   - results/features/ (for feature analysis)")
print("=" * 80)


print("\n" + "=" * 80)
print("ðŸ“š PART 1: LOAD DATA & FEATURE PLANNING")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 1.1: Load Timesheet Data")
print("=" * 80)

print("\nâ±ï¸ Loading dataset from data/timesheet_data_export.csv...")

# Load the data
df = pd.read_csv('../data/timesheet_data_export.csv')

# Convert date column
df['log_date'] = pd.to_datetime(df['log_date'])

print("\nâœ… Dataset loaded successfully!")
print("=" * 80)

print("\nðŸ“Š DATASET OVERVIEW:")
print("=" * 80)
print(f"Total Records: {len(df)}")
print(f"Date Range: {df['log_date'].min()} to {df['log_date'].max()}")
print(f"Unique Users: {df['user_id'].nunique()}")
print(f"Unique Activities: {df['activity_id'].nunique()}")

print("\nðŸ“‹ FIRST 5 RECORDS:")
print("=" * 80)
df.head()


print("\n" + "=" * 80)
print("EXERCISE 1.2: ML Feature Engineering Plan")
print("=" * 80)

print("""
ðŸŽ¯ FEATURES TO ENGINEER FOR RANDOM FOREST:

1. TEMPORAL FEATURES (From log_date):
   âœ“ day_of_week          â†’ Day name (Monday, Tuesday, ...)
   âœ“ day_of_week_encoded  â†’ Numerical (0-6)
   âœ“ hour_of_day          â†’ Hour when activity started (0-23)
   âœ“ month                â†’ Month number (1-12)
   âœ“ is_month_end         â†’ Boolean (True if day >= 25)
   âœ“ week_of_year         â†’ Week number (1-52)

2. PATTERN FEATURES (Calculated from history):
   âœ“ activity_frequency       â†’ How often activity appears on this day
   âœ“ avg_duration             â†’ Average hours for this activity
   âœ“ duration_consistency     â†’ Stability of duration (1 - CV)
   âœ“ recent_trend             â†’ Change in occurrence (last week vs before)
   âœ“ total_occurrences        â†’ Total count in historical data

3. CONTEXTUAL FEATURES (From data):
   âœ“ is_billable          â†’ Binary (0 or 1)
   âœ“ activity_id          â†’ Activity identifier
   âœ“ user_id_encoded      â†’ User identifier (encoded)

4. TARGET VARIABLE:
   âœ“ should_predict       â†’ Binary: 1 if activity should be predicted, 0 otherwise
                            (Based on frequency threshold: freq >= 0.7)

ðŸ“Š TOTAL FEATURES: 14
ðŸŽ¯ TARGET: should_predict (Binary Classification)
""")

print("=" * 80)


print("\n" + "=" * 80)
print("ðŸ“š PART 2: TEMPORAL FEATURE ENGINEERING")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 2.1: Extract Temporal Features")
print("=" * 80)

# Extract day of week
df['day_of_week'] = df['log_date'].dt.day_name()
df['day_of_week_encoded'] = df['log_date'].dt.dayofweek  # 0=Monday, 6=Sunday

# Extract month
df['month'] = df['log_date'].dt.month

# Extract week of year
df['week_of_year'] = df['log_date'].dt.isocalendar().week

# Month-end indicator
df['is_month_end'] = (df['log_date'].dt.day >= 25).astype(int)

# Extract hour from start_time
def extract_hour(time_str):
    try:
        parts = str(time_str).split()
        if len(parts) >= 4:
            time_part = parts[4]
            hour = int(time_part.split(':')[0])
            return hour
    except:
        pass
    return 9  # Default to 9 AM if parsing fails

df['hour_of_day'] = df['start_time'].apply(extract_hour)

print("\nâœ… Temporal features extracted!")
print("=" * 80)

print("\nðŸ“Š TEMPORAL FEATURES SUMMARY:")
print("=" * 80)
print(f"Day of Week Range: {df['day_of_week'].min()} to {df['day_of_week'].max()}")
print(f"Month Range: {df['month'].min()} to {df['month'].max()}")
print(f"Hour Range: {df['hour_of_day'].min()} to {df['hour_of_day'].max()}")
print(f"Month-End Records: {df['is_month_end'].sum()} ({(df['is_month_end'].sum()/len(df)*100):.1f}%)")

print("\nðŸ“‹ SAMPLE OF TEMPORAL FEATURES:")
print("=" * 80)
df[['log_date', 'day_of_week', 'day_of_week_encoded', 'hour_of_day', 'month', 'is_month_end']].head(10)


print("\n" + "=" * 80)
print("EXERCISE 2.2: Visualize Temporal Feature Distributions")
print("=" * 80)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Day of week distribution
day_counts = df['day_of_week_encoded'].value_counts().sort_index()
day_counts.plot(kind='bar', ax=axes[0, 0], color='skyblue', edgecolor='black')
axes[0, 0].set_title('Distribution by Day of Week', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Day (0=Monday, 6=Sunday)', fontsize=12)
axes[0, 0].set_ylabel('Count', fontsize=12)

# Set labels only for days that exist in the data
day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
existing_labels = [day_names[int(day)] for day in day_counts.index]
axes[0, 0].set_xticklabels(existing_labels, rotation=0)
axes[0, 0].grid(alpha=0.3, axis='y')

# Hour distribution
df['hour_of_day'].value_counts().sort_index().plot(
    kind='bar', ax=axes[0, 1], color='lightcoral', edgecolor='black'
)
axes[0, 1].set_title('Distribution by Hour of Day', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Hour', fontsize=12)
axes[0, 1].set_ylabel('Count', fontsize=12)
axes[0, 1].grid(alpha=0.3, axis='y')

# Month distribution
df['month'].value_counts().sort_index().plot(
    kind='bar', ax=axes[1, 0], color='lightgreen', edgecolor='black'
)
axes[1, 0].set_title('Distribution by Month', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Month', fontsize=12)
axes[1, 0].set_ylabel('Count', fontsize=12)
axes[1, 0].grid(alpha=0.3, axis='y')

# Month-end indicator
month_end_counts = df['is_month_end'].value_counts().sort_index()
month_end_counts.plot(kind='bar', ax=axes[1, 1], color='plum', edgecolor='black')
axes[1, 1].set_title('Month-End vs Regular Days', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Is Month End', fontsize=12)
axes[1, 1].set_ylabel('Count', fontsize=12)

# Set labels dynamically based on what exists
month_end_labels = []
for val in month_end_counts.index:
    month_end_labels.append('Month End' if val == 1 else 'Regular Day')
axes[1, 1].set_xticklabels(month_end_labels, rotation=0)
axes[1, 1].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('../results/features/temporal_features.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/features/temporal_features.png")


print("\n" + "=" * 80)
print("ðŸ“š PART 3: PATTERN FEATURE ENGINEERING")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 3.1: Calculate Pattern-Based Features")
print("=" * 80)

# Initialize pattern features
pattern_features = []

for idx, row in df.iterrows():
    activity_id = row['activity_id']
    day_of_week = row['day_of_week_encoded']
    
    # Get all historical records for this activity
    activity_history = df[df['activity_id'] == activity_id]
    
    # 1. Total occurrences
    total_occurrences = len(activity_history)
    
    # 2. Activity frequency on this day of week
    same_day_history = activity_history[activity_history['day_of_week_encoded'] == day_of_week]
    total_days_of_week = len(df[df['day_of_week_encoded'] == day_of_week])
    activity_frequency = len(same_day_history) / max(total_days_of_week, 1)
    
    # 3. Average duration
    avg_duration = activity_history['total_hours'].mean()
    
    # 4. Duration consistency (1 - coefficient of variation)
    std_duration = activity_history['total_hours'].std()
    if pd.notna(std_duration) and avg_duration > 0:
        duration_consistency = 1 - (std_duration / avg_duration)
        duration_consistency = max(0, min(1, duration_consistency))  # Clip to [0, 1]
    else:
        duration_consistency = 1.0 if total_occurrences == 1 else 0.0
    
    # 5. Recent trend (last week vs previous)
    current_date = row['log_date']
    week_ago = current_date - timedelta(days=7)
    
    recent_count = len(activity_history[activity_history['log_date'] >= week_ago])
    older_count = len(activity_history[activity_history['log_date'] < week_ago])
    
    if older_count > 0:
        recent_trend = (recent_count - older_count) / older_count
    else:
        recent_trend = 0.0
    
    pattern_features.append({
        'total_occurrences': total_occurrences,
        'activity_frequency': round(activity_frequency, 4),
        'avg_duration': round(avg_duration, 4),
        'duration_consistency': round(duration_consistency, 4),
        'recent_trend': round(recent_trend, 4)
    })

# Add pattern features to dataframe
pattern_df = pd.DataFrame(pattern_features)
df = pd.concat([df, pattern_df], axis=1)

print("\nâœ… Pattern features calculated!")
print("=" * 80)

print("\nðŸ“Š PATTERN FEATURES SUMMARY:")
print("=" * 80)
print(df[['activity_name', 'total_occurrences', 'activity_frequency', 
         'avg_duration', 'duration_consistency', 'recent_trend']].describe().round(4))

print("\nðŸ“‹ SAMPLE PATTERN FEATURES:")
print("=" * 80)
df[['activity_name', 'day_of_week', 'activity_frequency', 'duration_consistency', 'recent_trend']].head(10)


print("\n" + "=" * 80)
print("EXERCISE 3.2: Visualize Pattern Feature Distributions")
print("=" * 80)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Activity frequency distribution
axes[0, 0].hist(df['activity_frequency'], bins=20, edgecolor='black', color='skyblue')
axes[0, 0].set_title('Activity Frequency Distribution', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Frequency Score', fontsize=12)
axes[0, 0].set_ylabel('Count', fontsize=12)
axes[0, 0].grid(alpha=0.3, axis='y')
axes[0, 0].axvline(0.7, color='red', linestyle='--', linewidth=2, label='Prediction Threshold (0.7)')
axes[0, 0].legend()

# Duration consistency distribution
axes[0, 1].hist(df['duration_consistency'], bins=20, edgecolor='black', color='lightcoral')
axes[0, 1].set_title('Duration Consistency Distribution', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Consistency Score', fontsize=12)
axes[0, 1].set_ylabel('Count', fontsize=12)
axes[0, 1].grid(alpha=0.3, axis='y')

# Recent trend distribution
axes[1, 0].hist(df['recent_trend'], bins=20, edgecolor='black', color='lightgreen')
axes[1, 0].set_title('Recent Trend Distribution', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Trend Score', fontsize=12)
axes[1, 0].set_ylabel('Count', fontsize=12)
axes[1, 0].grid(alpha=0.3, axis='y')
axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=1, label='No Change')
axes[1, 0].legend()

# Total occurrences distribution
axes[1, 1].hist(df['total_occurrences'], bins=20, edgecolor='black', color='plum')
axes[1, 1].set_title('Total Occurrences Distribution', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Occurrence Count', fontsize=12)
axes[1, 1].set_ylabel('Count', fontsize=12)
axes[1, 1].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('../results/features/pattern_features.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/features/pattern_features.png")

print("\nðŸ’¡ PATTERN FEATURE INSIGHTS:")
print("=" * 80)
high_freq = len(df[df['activity_frequency'] >= 0.7])
print(f"High Frequency Activities (â‰¥0.7): {high_freq} ({(high_freq/len(df)*100):.1f}%)")
high_consistency = len(df[df['duration_consistency'] >= 0.7])
print(f"High Consistency Activities (â‰¥0.7): {high_consistency} ({(high_consistency/len(df)*100):.1f}%)")
increasing_trend = len(df[df['recent_trend'] > 0])
print(f"Increasing Trend Activities: {increasing_trend} ({(increasing_trend/len(df)*100):.1f}%)")


print("\n" + "=" * 80)
print("ðŸ“š PART 4: FEATURE ENCODING & NORMALIZATION")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 4.1: Encode Categorical Features")
print("=" * 80)

# Encode user_id
le_user = LabelEncoder()
df['user_id_encoded'] = le_user.fit_transform(df['user_id'])

print("\nâœ… Categorical features encoded!")
print("=" * 80)

print("\nðŸ“Š ENCODING SUMMARY:")
print("=" * 80)
print(f"Original user_id values: {df['user_id'].nunique()}")
print(f"Encoded user_id range: {df['user_id_encoded'].min()} to {df['user_id_encoded'].max()}")

print("\nðŸ“‹ ENCODING MAPPING (First 5):")
print("=" * 80)
user_mapping = df[['user_id', 'user_id_encoded']].drop_duplicates().head()
print(user_mapping.to_string(index=False))


print("\n" + "=" * 80)
print("EXERCISE 4.2: Create Target Variable")
print("=" * 80)

# Use median frequency as threshold for small dataset
# This ensures ~50% positive and ~50% negative examples
FREQUENCY_THRESHOLD = df['activity_frequency'].median()

df['should_predict'] = (df['activity_frequency'] >= FREQUENCY_THRESHOLD).astype(int)

print(f"\nâœ… Target variable created (threshold: {FREQUENCY_THRESHOLD:.4f})!")
print("   Using MEDIAN frequency to ensure balanced classes")
print("=" * 80)

print("\nðŸŽ¯ TARGET DISTRIBUTION:")
print("=" * 80)
target_counts = df['should_predict'].value_counts()
print(f"Should NOT Predict (0): {target_counts.get(0, 0)} ({(target_counts.get(0, 0)/len(df)*100):.1f}%)")
print(f"Should Predict (1): {target_counts.get(1, 0)} ({(target_counts.get(1, 0)/len(df)*100):.1f}%)")

# Visualize
plt.figure(figsize=(8, 5))
target_counts.plot(kind='bar', color=['#ff6b6b', '#4ecdc4'], edgecolor='black')
plt.title(f'Target Variable Distribution (Threshold: {FREQUENCY_THRESHOLD:.4f})', fontsize=14, fontweight='bold')
plt.xlabel('Should Predict', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks([0, 1], ['No (0)', 'Yes (1)'], rotation=0)
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('../results/features/target_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/features/target_distribution.png")

print("\nðŸ’¡ TARGET BALANCE:")
print("=" * 80)

if target_counts.get(0, 0) > 0 and target_counts.get(1, 0) > 0:
    balance_ratio = target_counts.get(1, 0) / target_counts.get(0, 0)
    if balance_ratio > 0.8 and balance_ratio < 1.2:
        print("âœ… Dataset is BALANCED (ratio close to 1:1)")
        print(f"   Ratio (1:0): {balance_ratio:.2f}")
    elif balance_ratio > 0.5 and balance_ratio < 2.0:
        print("âš ï¸ Dataset is SLIGHTLY IMBALANCED but acceptable for training")
        print(f"   Ratio (1:0): {balance_ratio:.2f}")
    else:
        print("âŒ Dataset is IMBALANCED")
        print(f"   Ratio (1:0): {balance_ratio:.2f}")
        print("   Recommendation: Use class_weight='balanced' in Random Forest")
else:
    print("âŒ TARGET CREATION FAILED - Check frequency distribution")

print("\nðŸ“Š FREQUENCY DISTRIBUTION:")
print("=" * 80)
print(f"Min frequency: {df['activity_frequency'].min():.4f}")
print(f"25th percentile: {df['activity_frequency'].quantile(0.25):.4f}")
print(f"50th percentile (median): {df['activity_frequency'].quantile(0.50):.4f} â† THRESHOLD")
print(f"75th percentile: {df['activity_frequency'].quantile(0.75):.4f}")
print(f"Max frequency: {df['activity_frequency'].max():.4f}")

print("\nðŸ’¡ INTERPRETATION:")
print("=" * 80)
print(f"Activities with frequency â‰¥ {FREQUENCY_THRESHOLD:.4f} will be predicted")
print(f"This represents the top 50% most frequent activities")


print("\n" + "=" * 80)
print("ðŸ“š PART 5: DATASET CREATION & EXPORT")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 5.1: Select Final Feature Set")
print("=" * 80)

# Define final feature columns for ML
feature_columns = [
    # Temporal features
    'day_of_week_encoded',
    'hour_of_day',
    'month',
    'is_month_end',
    
    # Pattern features
    'activity_frequency',
    'avg_duration',
    'duration_consistency',
    'recent_trend',
    'total_occurrences',
    
    # Contextual features
    'is_billable',
    'activity_id',
    'user_id_encoded'
]

target_column = 'should_predict'

# Create final dataset
X = df[feature_columns].copy()
y = df[target_column].copy()

print("\nâœ… Feature set defined!")
print("=" * 80)

print("\nðŸ“Š FINAL FEATURE SET:")
print("=" * 80)
print(f"Total Features: {len(feature_columns)}")
print(f"Target Variable: {target_column}")

print("\nðŸ“‹ FEATURE LIST:")
print("=" * 80)
for i, feat in enumerate(feature_columns, 1):
    print(f"{i:2d}. {feat}")

print("\nðŸ“ˆ FEATURE STATISTICS:")
print("=" * 80)
print(X.describe().round(4))


print("\n" + "=" * 80)
print("EXERCISE 5.2: Save Processed Dataset")
print("=" * 80)

# Combine features and target
final_dataset = X.copy()
final_dataset['target'] = y

# Save to CSV
output_path = '../data/processed/features_engineered.csv'
final_dataset.to_csv(output_path, index=False)

print(f"\nâœ… Dataset saved to: {output_path}")
print("=" * 80)

print("\nðŸ“Š SAVED DATASET INFO:")
print("=" * 80)
print(f"Total Rows: {len(final_dataset)}")
print(f"Total Columns: {len(final_dataset.columns)}")
print(f"Features: {len(feature_columns)}")
print(f"Target: 1 column ({target_column})")
print(f"File Size: {os.path.getsize(output_path) / 1024:.2f} KB")

print("\nðŸ“‹ SAMPLE OF SAVED DATA:")
print("=" * 80)
final_dataset.head(10)


print("\n" + "=" * 80)
print("EXERCISE 5.3: Feature Correlation with Target")
print("=" * 80)

# Calculate correlation with target
correlations = X.corrwith(y).sort_values(ascending=False)

print("\nðŸ“Š FEATURE CORRELATION WITH TARGET:")
print("=" * 80)
for feat, corr in correlations.items():
    print(f"{feat:30s}: {corr:+.4f}")

# Visualize
plt.figure(figsize=(10, 8))
correlations.plot(kind='barh', color=['green' if x > 0 else 'red' for x in correlations])
plt.title('Feature Correlation with Target', fontsize=14, fontweight='bold')
plt.xlabel('Correlation Coefficient', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.axvline(0, color='black', linewidth=0.8)
plt.grid(alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('../results/features/feature_correlations.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/features/feature_correlations.png")

print("\nðŸ’¡ CORRELATION INSIGHTS:")
print("=" * 80)
top_positive = correlations.head(3)
print("\nTop 3 Positive Correlations:")
for feat, corr in top_positive.items():
    print(f"   â€¢ {feat}: {corr:+.4f}")

if (correlations < 0).any():
    top_negative = correlations.tail(3)
    print("\nTop 3 Negative Correlations:")
    for feat, corr in top_negative.items():
        print(f"   â€¢ {feat}: {corr:+.4f}")
        


print("\n" + "=" * 80)
print("ðŸŽ¯ PHASE 2 - FEATURE ENGINEERING COMPLETE!")
print("=" * 80)

print("""
ðŸ“š WHAT I ACCOMPLISHED TODAY:

âœ… PART 1: Data Loading & Planning
   â€¢ Loaded 44 timesheet records
   â€¢ Defined 14 ML features to engineer

âœ… PART 2: Temporal Feature Engineering
   â€¢ Extracted day_of_week, hour, month features
   â€¢ Created month-end indicator
   â€¢ Generated temporal visualizations

âœ… PART 3: Pattern Feature Engineering
   â€¢ Calculated activity frequency scores
   â€¢ Measured duration consistency
   â€¢ Computed recent trend indicators
   â€¢ Counted total occurrences

âœ… PART 4: Feature Encoding
   â€¢ Encoded categorical user_id
   â€¢ Created binary target variable
   â€¢ Analyzed target distribution

âœ… PART 5: Dataset Creation
   â€¢ Selected 12 final features
   â€¢ Combined with target variable
   â€¢ Saved to features_engineered.csv
   â€¢ Analyzed feature-target correlations

ðŸ“Š FEATURE ENGINEERING SUMMARY:
""" + f"""
   â€¢ Total Features: {len(feature_columns)}
   â€¢ Total Records: {len(X)}
   â€¢ Target Balance: {(y.sum()/len(y)*100):.1f}% positive class
   â€¢ Saved Dataset: data/processed/features_engineered.csv

ðŸ’¾ FILES CREATED TODAY:
   â€¢ data/processed/features_engineered.csv
   â€¢ results/features/temporal_features.png
   â€¢ results/features/pattern_features.png
   â€¢ results/features/target_distribution.png
   â€¢ results/features/feature_correlations.png

ðŸŽ¯ NEXT STEPS:

PHASE 3 - Random Forest Training:
   1. Load engineered features
   2. Split train/test sets (70/30)
   3. Train Random Forest classifier
   4. Evaluate prediction accuracy
   5. Analyze feature importance
   6. Save trained model

ðŸ“ˆ FEATURES READY FOR ML:
   âœ… Temporal: day_of_week, hour, month, is_month_end
   âœ… Pattern: frequency, consistency, trend, occurrences
   âœ… Context: is_billable, activity_id, user_id
   âœ… Target: should_predict (binary)

ðŸ”œ NEXT NOTEBOOK: 03_model_training.ipynb
   Focus: Train and evaluate Random Forest model
""")

print("=" * 80)
print("âœ… Phase 2 Feature Engineering Complete!")
print("ðŸš€ Ready to proceed to Model Training!")
print("=" * 80)



