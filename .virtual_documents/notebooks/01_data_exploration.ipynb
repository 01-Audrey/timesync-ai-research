"""
==================================================
TIMESYNC AI RESEARCH - SMART TIMESHEET PRE-FILL
==================================================
Project: OrasSync 2.0 - Smart Timesheet Pre-fill Feature
Sprint: ML Research Phase
Current Week: 1
Date: February 15, 2026
Notebook: Data Exploration & Pattern Analysis

ðŸ“Š PROJECT PHASES:
ðŸ”„ Phase 1: Data Exploration (IN PROGRESS)
â¬œ Phase 2: Feature Engineering
â¬œ Phase 3: Random Forest Model Training
â¬œ Phase 4: Model Evaluation
â¬œ Phase 5: Production Integration

ðŸŽ¯ PHASE 1 DELIVERABLES:
âœ… Export real OrasSync timesheet data (COMPLETED)
ðŸ”„ Data exploration (THIS NOTEBOOK)
â¬œ Pattern analysis
â¬œ Feature engineering planning

Progress: 33% (1/3 tasks)

ðŸŽ¯ THIS NOTEBOOK'S OBJECTIVES:
1. Load real OrasSync timesheet data (44 records)
2. Analyze activity patterns by day of week
3. Identify recurring activities and durations
4. Measure pattern consistency
5. Calculate predictability scores

ðŸ“š NOTEBOOK STRUCTURE:
Part 1: Data Loading & Initial Exploration
Part 2: Temporal Pattern Analysis
Part 3: Activity Frequency & Duration Analysis
Part 4: Predictability Scoring & Next Steps

âœ… SUCCESS CRITERIA:
âœ… Understand all timesheet features
âœ… Identify recurring patterns
âœ… Calculate confidence scores
âœ… Define ML feature requirements
âœ… Plan Random Forest training approach

==================================================
"""


# ==================================================
# INSTALL REQUIRED LIBRARIES
# ==================================================
import sys
get_ipython().getoutput("{sys.executable} -m pip install pandas numpy matplotlib seaborn scikit-learn -q")
print("âœ… Libraries installed!")
print("\n" + "=" * 80)

# ==================================================
# IMPORT LIBRARIES
# ==================================================
print("\n" + "=" * 80)
print("ðŸ“š IMPORTING LIBRARIES")
print("=" * 80)

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)

# Set plot style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("\nâœ… All libraries imported successfully!")
print("=" * 80)

# ==================================================
# CREATE OUTPUT DIRECTORIES
# ==================================================
print("\n" + "=" * 80)
print("ðŸ“ CREATING OUTPUT DIRECTORIES")
print("=" * 80)

os.makedirs('data/processed', exist_ok=True)
os.makedirs('results', exist_ok=True)
os.makedirs('results/plots', exist_ok=True)

print("\nâœ… Directories created:")
print("   - data/processed/ (for cleaned data)")
print("   - results/ (for analysis outputs)")
print("   - results/plots/ (for visualizations)")
print("=" * 80)


print("\n" + "=" * 80)
print("ðŸ“š PART 1: DATA LOADING & INITIAL EXPLORATION")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 1.1: Load OrasSync Timesheet Data")
print("=" * 80)

print("\nâ±ï¸ Loading dataset from data/timesheet_data_export.csv...")

# Load the real OrasSync data
df = pd.read_csv('../data/timesheet_data_export.csv')

print("\nâœ… Dataset loaded successfully!")
print("=" * 80)

print("\nðŸ“Š DATASET OVERVIEW:")
print("=" * 80)
print(f"Total Records: {len(df)}")
print(f"Total Features: {len(df.columns)}")
print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

print("\nðŸ“‹ FIRST 10 RECORDS:")
print("=" * 80)
df.head(10)


print("\n" + "=" * 80)
print("EXERCISE 1.2: Examine Data Structure")
print("=" * 80)

print("\nðŸ“Š COLUMN INFORMATION:")
print("=" * 80)
df.info()

print("\nðŸ“‹ COLUMN NAMES:")
print("=" * 80)
for i, col in enumerate(df.columns, 1):
    print(f"{i:2d}. {col}")

print("\nðŸ“Š DATA TYPES:")
print("=" * 80)
print(df.dtypes)

print("\nðŸ“ˆ UNIQUE VALUES PER COLUMN:")
print("=" * 80)
for col in df.columns:
    print(f"{col:30s}: {df[col].nunique():4d} unique values")


print("\n" + "=" * 80)
print("EXERCISE 1.3: Descriptive Statistics")
print("=" * 80)

print("\nðŸ“Š NUMERICAL FEATURES SUMMARY:")
print("=" * 80)
df.describe()


print("\n" + "=" * 80)
print("EXERCISE 1.4: Data Quality Assessment")
print("=" * 80)

print("\nðŸ” CHECKING FOR MISSING VALUES:")
print("=" * 80)
missing_values = df.isnull().sum()
missing_pct = (df.isnull().sum() / len(df)) * 100

missing_df = pd.DataFrame({
    'Column': missing_values.index,
    'Missing Count': missing_values.values,
    'Missing %': missing_pct.values
})
missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)

if len(missing_df) > 0:
    print(missing_df.to_string(index=False))
else:
    print("âœ… No missing values found!")

print("\nðŸ” CHECKING FOR DUPLICATE RECORDS:")
print("=" * 80)
duplicates = df.duplicated().sum()
print(f"Total Duplicate Records: {duplicates}")
if duplicates > 0:
    print(f"Percentage: {(duplicates/len(df))*100:.2f}%")
else:
    print("âœ… No duplicate records found!")

print("\nðŸ“Š DATA COMPLETENESS SCORE:")
print("=" * 80)
completeness = (1 - (df.isnull().sum().sum() / (len(df) * len(df.columns)))) * 100
print(f"Overall Data Completeness: {completeness:.2f}%")


print("\n" + "=" * 80)
print("ðŸ“š PART 2: TEMPORAL PATTERN ANALYSIS")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 2.1: Extract Temporal Features")
print("=" * 80)

# Convert log_date to datetime
df['log_date'] = pd.to_datetime(df['log_date'])

# Extract temporal features
df['day_of_week'] = df['log_date'].dt.day_name()
df['day_num'] = df['log_date'].dt.dayofweek  # 0=Monday, 6=Sunday
df['month'] = df['log_date'].dt.month
df['week_of_year'] = df['log_date'].dt.isocalendar().week

print("\nâœ… Temporal features extracted!")
print("=" * 80)

print("\nðŸ“Š TEMPORAL FEATURE SUMMARY:")
print("=" * 80)
print(f"Date Range: {df['log_date'].min()} to {df['log_date'].max()}")
print(f"Total Days Covered: {(df['log_date'].max() - df['log_date'].min()).days}")
print(f"Unique Users: {df['user_id'].nunique()}")
print(f"Unique Activities: {df['activity_id'].nunique()}")

print("\nðŸ“‹ ACTIVITY BREAKDOWN:")
print("=" * 80)
activity_counts = df.groupby('activity_name').size().sort_values(ascending=False)
for activity, count in activity_counts.items():
    print(f"{activity:30s}: {count:3d} entries")


print("\n" + "=" * 80)
print("EXERCISE 2.2: Activity Distribution by Day of Week")
print("=" * 80)

# Activity frequency by day of week
activity_by_day = df.groupby(['day_of_week', 'activity_name']).size().unstack(fill_value=0)

print("\nðŸ“Š ACTIVITY FREQUENCY BY DAY:")
print("=" * 80)
print(activity_by_day)

# Plot
plt.figure(figsize=(14, 6))
activity_by_day.plot(kind='bar', colormap='viridis')
plt.title('Activity Frequency by Day of Week', fontsize=16, fontweight='bold')
plt.xlabel('Day of Week', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.legend(title='Activity', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('results/plots/activity_by_day.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/plots/activity_by_day.png")


print("\n" + "=" * 80)
print("ðŸ“š PART 3: ACTIVITY FREQUENCY & DURATION ANALYSIS")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 3.1: Activity Duration Statistics")
print("=" * 80)

# Duration statistics per activity
duration_stats = df.groupby('activity_name')['total_hours'].agg([
    'count', 'mean', 'std', 'min', 'max'
]).round(2)

# Calculate consistency score (1 - coefficient of variation)
duration_stats['consistency'] = (1 - (duration_stats['std'] / duration_stats['mean'])).clip(0, 1).round(2)
duration_stats['consistency'] = duration_stats['consistency'].fillna(0)

print("\nðŸ“Š DURATION STATISTICS BY ACTIVITY:")
print("=" * 80)
print(duration_stats)

print("\nðŸ’¡ INTERPRETATION:")
print("   â€¢ Higher consistency score = More predictable duration")
print("   â€¢ Consistency near 1.0 = Very stable")
print("   â€¢ Consistency near 0.0 = Highly variable")


print("\n" + "=" * 80)
print("EXERCISE 3.2: Hourly Distribution Analysis")
print("=" * 80)

# Plot duration distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
df['total_hours'].hist(bins=20, edgecolor='black')
plt.title('Distribution of Activity Durations', fontsize=14, fontweight='bold')
plt.xlabel('Hours', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
df.boxplot(column='total_hours', by='activity_name', figsize=(12, 5))
plt.title('Activity Duration by Type', fontsize=14, fontweight='bold')
plt.xlabel('Activity', fontsize=12)
plt.ylabel('Hours', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.suptitle('')  # Remove default title

plt.tight_layout()
plt.savefig('results/plots/duration_distributions.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/plots/duration_distributions.png")

print("\nðŸ“Š DURATION INSIGHTS:")
print("=" * 80)
print(f"Average Duration: {df['total_hours'].mean():.2f} hours")
print(f"Median Duration: {df['total_hours'].median():.2f} hours")
print(f"Most Common Duration: {df['total_hours'].mode()[0]:.2f} hours")
print(f"Std Deviation: {df['total_hours'].std():.2f} hours")


print("\n" + "=" * 80)
print("EXERCISE 3.3: Billable vs Non-Billable Analysis")
print("=" * 80)

# Billable analysis
billable_counts = df.groupby('is_billable')['activity_name'].value_counts()

print("\nðŸ“Š BILLABLE vs NON-BILLABLE BREAKDOWN:")
print("=" * 80)

billable_hours = df.groupby('is_billable')['total_hours'].sum()
print(f"\nBillable Hours: {billable_hours.get(1, 0):.2f}")
print(f"Non-Billable Hours: {billable_hours.get(0, 0):.2f}")
print(f"Total Hours: {billable_hours.sum():.2f}")

if billable_hours.sum() > 0:
    billable_pct = (billable_hours.get(1, 0) / billable_hours.sum()) * 100
    print(f"Billable Percentage: {billable_pct:.1f}%")

# Visualization
plt.figure(figsize=(10, 5))
billable_hours.plot(kind='bar', color=['#ff6b6b', '#4ecdc4'])
plt.title('Total Hours: Billable vs Non-Billable', fontsize=14, fontweight='bold')
plt.xlabel('Billable Status', fontsize=12)
plt.ylabel('Total Hours', fontsize=12)
plt.xticks([0, 1], ['Non-Billable', 'Billable'], rotation=0)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('results/plots/billable_breakdown.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/plots/billable_breakdown.png")


print("\n" + "=" * 80)
print("EXERCISE 3.4: Feature Correlation Analysis")
print("=" * 80)

# Select numerical columns for correlation
numerical_cols = ['activity_id', 'total_hours', 'is_billable', 'day_num', 'month']
correlation_data = df[numerical_cols].corr()

print("\nðŸ“Š CORRELATION MATRIX:")
print("=" * 80)
print(correlation_data.round(3))

# Visualization
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_data, annot=True, fmt='.3f', cmap='coolwarm', 
            center=0, square=True, linewidths=1, cbar_kws={'label': 'Correlation Coefficient'})
plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('results/plots/correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/plots/correlation_matrix.png")

print("\nðŸ’¡ CORRELATION INSIGHTS:")
print("=" * 80)
# Find strongest correlations (excluding diagonal)
strong_corr = []
for i in range(len(correlation_data.columns)):
    for j in range(i+1, len(correlation_data.columns)):
        corr_value = correlation_data.iloc[i, j]
        if abs(corr_value) > 0.3:
            strong_corr.append({
                'Feature 1': correlation_data.columns[i],
                'Feature 2': correlation_data.columns[j],
                'Correlation': corr_value
            })

if strong_corr:
    strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlation', key=abs, ascending=False)
    print("\nStrong Correlations (|r| > 0.3):")
    print(strong_corr_df.to_string(index=False))
else:
    print("\nâœ… No strong correlations found (|r| > 0.3)")
    print("   â†’ Features are relatively independent")
    print("   â†’ Good for machine learning!")


print("\n" + "=" * 80)
print("EXERCISE 3.5: Time-of-Day Pattern Analysis")
print("=" * 80)

# Extract hour from start_time
# Format is like "Thu Jan 01 1970 23:50:31 GMT+0800"
def extract_hour(time_str):
    try:
        # Extract hour from the time string
        parts = str(time_str).split()
        if len(parts) >= 4:
            time_part = parts[4]  # "HH:MM:SS"
            hour = int(time_part.split(':')[0])
            return hour
    except:
        pass
    return None

df['hour_of_day'] = df['start_time'].apply(extract_hour)

print("\nâ° ACTIVITY START TIME DISTRIBUTION:")
print("=" * 80)

if df['hour_of_day'].notna().any():
    hour_dist = df['hour_of_day'].value_counts().sort_index()
    
    for hour, count in hour_dist.items():
        print(f"{int(hour):02d}:00 - {count:3d} activities")
    
    # Visualization
    plt.figure(figsize=(14, 5))
    
    plt.subplot(1, 2, 1)
    hour_dist.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.title('Activity Start Times Distribution', fontsize=14, fontweight='bold')
    plt.xlabel('Hour of Day', fontsize=12)
    plt.ylabel('Number of Activities', fontsize=12)
    plt.grid(alpha=0.3, axis='y')
    plt.xticks(rotation=0)
    
    plt.subplot(1, 2, 2)
    activity_by_hour = df.groupby(['hour_of_day', 'activity_name']).size().unstack(fill_value=0)
    activity_by_hour.plot(kind='bar', stacked=True, figsize=(14, 5))
    plt.title('Activities by Hour of Day (Stacked)', fontsize=14, fontweight='bold')
    plt.xlabel('Hour of Day', fontsize=12)
    plt.ylabel('Count', fontsize=12)
    plt.legend(title='Activity', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=0)
    
    plt.tight_layout()
    plt.savefig('results/plots/hourly_patterns.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nâœ… Visualization saved to: results/plots/hourly_patterns.png")
    
    print("\nðŸ’¡ TIME-OF-DAY INSIGHTS:")
    print("=" * 80)
    print(f"Most Active Hour: {hour_dist.idxmax():02.0f}:00 ({hour_dist.max()} activities)")
    print(f"Least Active Hour: {hour_dist.idxmin():02.0f}:00 ({hour_dist.min()} activities)")
    print(f"Average Activities/Hour: {hour_dist.mean():.1f}")
else:
    print("âš ï¸ Unable to extract hour information from start_time")
    print("   Time format may need adjustment for deeper analysis")


print("\n" + "=" * 80)
print("EXERCISE 3.6: Week-over-Week Trend Analysis")
print("=" * 80)

# Activity counts by week
weekly_activity = df.groupby(['week_of_year', 'activity_name']).size().unstack(fill_value=0)

print("\nðŸ“ˆ WEEKLY ACTIVITY TRENDS:")
print("=" * 80)
print(weekly_activity)

# Total hours by week
weekly_hours = df.groupby('week_of_year')['total_hours'].sum()

print("\nâ±ï¸ TOTAL HOURS BY WEEK:")
print("=" * 80)
for week, hours in weekly_hours.items():
    print(f"Week {int(week):2d}: {hours:6.1f} hours")

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# Activity counts over time
weekly_activity.plot(ax=axes[0], marker='o', linewidth=2)
axes[0].set_title('Activity Trends Over Time', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Week of Year', fontsize=12)
axes[0].set_ylabel('Activity Count', fontsize=12)
axes[0].legend(title='Activity', bbox_to_anchor=(1.05, 1), loc='upper left')
axes[0].grid(alpha=0.3)

# Total hours over time
weekly_hours.plot(ax=axes[1], marker='o', linewidth=2, color='orange')
axes[1].set_title('Total Hours Logged Over Time', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Week of Year', fontsize=12)
axes[1].set_ylabel('Total Hours', fontsize=12)
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('results/plots/weekly_trends.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ… Visualization saved to: results/plots/weekly_trends.png")

print("\nðŸ’¡ TREND INSIGHTS:")
print("=" * 80)
if len(weekly_hours) > 1:
    trend = "increasing" if weekly_hours.iloc[-1] > weekly_hours.iloc[0] else "decreasing"
    change = ((weekly_hours.iloc[-1] - weekly_hours.iloc[0]) / weekly_hours.iloc[0]) * 100
    print(f"Overall Trend: {trend.upper()}")
    print(f"Change from Week {int(weekly_hours.index[0])} to Week {int(weekly_hours.index[-1])}: {change:+.1f}%")
else:
    print("âš ï¸ Insufficient weeks for trend analysis")


print("\n" + "=" * 80)
print("ðŸ“š PART 4: FEATURE CATEGORIZATION & ML PLANNING")
print("=" * 80)


print("\n" + "=" * 80)
print("EXERCISE 4.1: Feature Categorization for Machine Learning")
print("=" * 80)

# Categorize features
temporal_features = ['log_date', 'day_of_week', 'day_num', 'month', 'week_of_year', 'hour_of_day']
numerical_features = ['activity_id', 'total_hours', 'is_billable']
categorical_features = ['user_id', 'activity_name', 'activity_code']
target_feature = 'activity_id'  # What we want to predict

print("\nðŸ“Š FEATURE CATEGORIZATION:")
print("=" * 80)

print(f"\nðŸ• TEMPORAL FEATURES ({len(temporal_features)}):")
for i, feat in enumerate(temporal_features, 1):
    print(f"   {i}. {feat}")

print(f"\nðŸ”¢ NUMERICAL FEATURES ({len(numerical_features)}):")
for i, feat in enumerate(numerical_features, 1):
    print(f"   {i}. {feat}")

print(f"\nðŸ“ CATEGORICAL FEATURES ({len(categorical_features)}):")
for i, feat in enumerate(categorical_features, 1):
    unique_count = df[feat].nunique()
    print(f"   {i}. {feat} ({unique_count} unique values)")

print(f"\nðŸŽ¯ TARGET FEATURE:")
print(f"   â†’ {target_feature}")
print(f"   â†’ Goal: Predict which activity should appear in timesheet")

print("\nðŸ’¡ ML FEATURE ENGINEERING PLAN:")
print("=" * 80)
print("""
For Random Forest Model Training:

1. TEMPORAL FEATURES (Engineered):
   â€¢ day_of_week â†’ One-hot encoding (Mon, Tue, Wed, ...)
   â€¢ hour_of_day â†’ Numerical (0-23)
   â€¢ month â†’ Numerical (1-12)
   â€¢ week_of_year â†’ Numerical

2. PATTERN FEATURES (From Analysis):
   â€¢ activity_frequency â†’ Calculated from historical data
   â€¢ duration_consistency â†’ From duration variance
   â€¢ recent_trend â†’ Change in occurrence rate
   â€¢ is_month_end â†’ Boolean (day >= 25)

3. HISTORICAL FEATURES (Aggregated):
   â€¢ total_occurrences â†’ Count of activity in history
   â€¢ avg_duration â†’ Mean duration for activity
   â€¢ last_occurrence_days â†’ Days since last occurrence

4. TARGET VARIABLE:
   â€¢ should_predict â†’ Binary (1 if activity should appear, 0 otherwise)
   â€¢ Threshold: Activities with frequency > 70% on that day
""")


print("\n" + "=" * 80)
print("EXERCISE 4.2: Calculate Predictability Scores")
print("=" * 80)

# Calculate predictability for each activity-day combination
predictability_data = []

for activity_name in df['activity_name'].unique():
    activity_df = df[df['activity_name'] == activity_name]
    
    for day in range(7):
        day_data = activity_df[activity_df['day_num'] == day]
        
        if len(day_data) > 0:
            # Calculate frequency (how often it appears on this day)
            total_days = len(df[df['day_num'] == day])
            frequency = len(day_data) / max(total_days, 1)
            
            # Calculate duration consistency
            avg_duration = day_data['total_hours'].mean()
            std_duration = day_data['total_hours'].std()
            
            if pd.notna(std_duration) and avg_duration > 0:
                consistency = 1 - (std_duration / avg_duration)
                consistency = max(0, consistency)
            else:
                consistency = 1.0 if len(day_data) == 1 else 0.0
            
            # Predictability score (weighted average)
            score = (frequency * 0.6) + (consistency * 0.4)
            
            day_name = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'][day]
            
            predictability_data.append({
                'activity_name': activity_name,
                'day_of_week': day_name,
                'day_num': day,
                'frequency': round(frequency, 2),
                'consistency': round(consistency, 2),
                'predictability_score': round(score, 2),
                'occurrences': len(day_data)
            })

pred_df = pd.DataFrame(predictability_data)

print("\nðŸ“Š TOP 10 MOST PREDICTABLE ACTIVITY-DAY COMBINATIONS:")
print("=" * 80)
top_predictable = pred_df.nlargest(10, 'predictability_score')[
    ['activity_name', 'day_of_week', 'frequency', 'consistency', 'predictability_score', 'occurrences']
]
print(top_predictable.to_string(index=False))

print("\nðŸ’¡ INTERPRETATION:")
print("   â€¢ Predictability Score Range: 0.0 (unpredictable) to 1.0 (highly predictable)")
print("   â€¢ Score = (Frequency Ã— 60%) + (Consistency Ã— 40%)")
print("   â€¢ High scores indicate activities likely to recur on specific days")


print("\n" + "=" * 80)
print("EXERCISE 4.3: Predictability Heatmap Visualization")
print("=" * 80)

# Create pivot table for heatmap
heatmap_data = pred_df.pivot_table(
    values='predictability_score',
    index='activity_name',
    columns='day_of_week',
    aggfunc='mean'
)

# ... rest of the code stays the same


print("\n" + "=" * 80)
print("EXERCISE 4.4: Comprehensive Data Quality Report")
print("=" * 80)

# Generate comprehensive quality report
quality_report = []

for col in df.columns:
    col_data = {
        'Feature': col,
        'Type': str(df[col].dtype),
        'Non-Null': df[col].notna().sum(),
        'Null Count': df[col].isna().sum(),
        'Null %': round((df[col].isna().sum() / len(df)) * 100, 2),
        'Unique': df[col].nunique(),
        'Completeness': round((df[col].notna().sum() / len(df)) * 100, 2)
    }
    quality_report.append(col_data)

quality_df = pd.DataFrame(quality_report)

print("\nðŸ“Š COMPLETE DATA QUALITY REPORT:")
print("=" * 80)
print(quality_df.to_string(index=False))

print("\nðŸ“ˆ OVERALL DATA QUALITY METRICS:")
print("=" * 80)
total_cells = len(df) * len(df.columns)
total_filled = quality_df['Non-Null'].sum()
overall_completeness = (total_filled / total_cells) * 100

print(f"Total Data Cells: {total_cells:,}")
print(f"Filled Cells: {total_filled:,}")
print(f"Missing Cells: {total_cells - total_filled:,}")
print(f"Overall Completeness: {overall_completeness:.2f}%")

# Quality grade
if overall_completeness >= 95:
    grade = "EXCELLENT âœ…"
elif overall_completeness >= 85:
    grade = "GOOD âœ“"
elif overall_completeness >= 75:
    grade = "FAIR âš ï¸"
else:
    grade = "POOR âŒ"

print(f"\nData Quality Grade: {grade}")


print("\n" + "=" * 80)
print("EXERCISE 4.5: Machine Learning Readiness Assessment")
print("=" * 80)

print("\nðŸŽ¯ RANDOM FOREST MODEL REQUIREMENTS CHECK:")
print("=" * 80)

# Dataset size check
min_records = 30
record_status = "âœ… PASS" if len(df) >= min_records else "âŒ FAIL"
print(f"\n1. Dataset Size:")
print(f"   Current: {len(df)} records")
print(f"   Minimum: {min_records} records")
print(f"   Status: {record_status}")

# Data completeness check
completeness_threshold = 90
completeness_status = "âœ… PASS" if overall_completeness >= completeness_threshold else "âŒ FAIL"
print(f"\n2. Data Completeness:")
print(f"   Current: {overall_completeness:.2f}%")
print(f"   Minimum: {completeness_threshold}%")
print(f"   Status: {completeness_status}")

# Feature diversity check
min_features = 5
feature_count = len(temporal_features) + len(numerical_features)
feature_status = "âœ… PASS" if feature_count >= min_features else "âŒ FAIL"
print(f"\n3. Feature Diversity:")
print(f"   Current: {feature_count} features")
print(f"   Minimum: {min_features} features")
print(f"   Status: {feature_status}")

# Pattern existence check
predictable_patterns = len(pred_df[pred_df['predictability_score'] > 0.5])
min_patterns = 5
pattern_status = "âœ… PASS" if predictable_patterns >= min_patterns else "âš ï¸ LIMITED"
print(f"\n4. Predictable Patterns:")
print(f"   Current: {predictable_patterns} patterns (score > 0.5)")
print(f"   Minimum: {min_patterns} patterns")
print(f"   Status: {pattern_status}")

# User coverage check
min_users = 2
user_count = df['user_id'].nunique()
user_status = "âœ… PASS" if user_count >= min_users else "âŒ FAIL"
print(f"\n5. User Coverage:")
print(f"   Current: {user_count} unique users")
print(f"   Minimum: {min_users} users")
print(f"   Status: {user_status}")

# Temporal coverage check
min_days = 7
day_span = (df['log_date'].max() - df['log_date'].min()).days
temporal_status = "âœ… PASS" if day_span >= min_days else "âš ï¸ LIMITED"
print(f"\n6. Temporal Coverage:")
print(f"   Current: {day_span} days")
print(f"   Minimum: {min_days} days")
print(f"   Status: {temporal_status}")

# Overall assessment
all_pass = all([
    len(df) >= min_records,
    overall_completeness >= completeness_threshold,
    feature_count >= min_features,
    user_count >= min_users
])

print("\n" + "=" * 80)
if all_pass:
    print("ðŸŽ¯ OVERALL ASSESSMENT: READY FOR ML TRAINING! âœ…")
    print("\nNext Steps:")
    print("   1. Feature engineering (extract ML features)")
    print("   2. Create training dataset (70% train, 30% test)")
    print("   3. Train Random Forest classifier")
    print("   4. Evaluate model performance")
else:
    print("âš ï¸ OVERALL ASSESSMENT: NEEDS MORE DATA")
    print("\nRecommendations:")
    print("   â€¢ Collect at least 2 weeks of timesheet data")
    print("   â€¢ Ensure multiple users are logging time")
    print("   â€¢ Verify data completeness across all features")
print("=" * 80)


print("\n" + "=" * 80)
print("ðŸŽ¯ PHASE 1 - DATA EXPLORATION COMPLETE!")
print("=" * 80)

print("""
ðŸ“š WHAT I ACCOMPLISHED TODAY:

âœ… PART 1: Data Loading & Exploration
   â€¢ Loaded OrasSync timesheet data (44 records)
   â€¢ Analyzed column structure and data types
   â€¢ Generated descriptive statistics

âœ… PART 2: Temporal Pattern Analysis
   â€¢ Extracted temporal features (day, week, month)
   â€¢ Visualized activity distribution by day of week
   â€¢ Identified day-specific patterns

âœ… PART 3: Activity Analysis
   â€¢ Calculated duration statistics per activity
   â€¢ Measured consistency scores
   â€¢ Identified most/least variable activities

âœ… PART 4: Predictability Scoring
   â€¢ Calculated frequency Ã— consistency scores
   â€¢ Identified highly predictable patterns
   â€¢ Ranked activity-day combinations

ðŸ“Š KEY STATISTICS:
   â€¢ Total records: 44
   â€¢ Unique users: """ + str(df['user_id'].nunique()) + """
   â€¢ Unique activities: """ + str(df['activity_id'].nunique()) + """
   â€¢ Date range: """ + str((df['log_date'].max() - df['log_date'].min()).days) + """ days
   â€¢ Billable activities: """ + str(df[df['is_billable'] == 1]['activity_name'].nunique()) + """

ðŸ’¡ KEY INSIGHTS:

1. PATTERN QUALITY
   â†’ """ + str(len(pred_df[pred_df['predictability_score'] > 0.7])) + """ highly predictable patterns found (score > 0.7)
   â†’ """ + str(len(pred_df[pred_df['frequency'] > 0.5])) + """ frequently recurring patterns (frequency > 50%)

2. NEXT STEPS FOR ML:
   â†’ Feature engineering: Extract day-of-week, hour, frequency, consistency
   â†’ Model training: Random Forest with 70/30 split
   â†’ Target: Binary classification (should activity appear today?)

ðŸŽ¯ NEXT STEPS:

PHASE 2 - Feature Engineering:
   1. Create engineered features (frequency, consistency, trends)
   2. Build feature vectors for ML
   3. Prepare training/test datasets

PHASE 3 - Model Training:
   4. Train Random Forest classifier
   5. Evaluate prediction accuracy
   6. Tune hyperparameters

ðŸ’¾ FILES CREATED TODAY:
   â€¢ results/plots/activity_by_day.png

ðŸ“Š ML READINESS ASSESSMENT:
   âœ… Dataset size: SUFFICIENT (44 records for initial model)
   âœ… Pattern existence: CONFIRMED (predictable patterns found)
   âœ… Feature diversity: GOOD (temporal + activity features)
   
   ðŸŽ¯ OVERALL: READY FOR FEATURE ENGINEERING!

ðŸ”œ TOMORROW'S FOCUS:
   Feature engineering notebook:
   â€¢ Extract ML features from patterns
   â€¢ Build Random Forest training pipeline
   â€¢ Create prediction confidence scoring

""")

print("=" * 80)
print("âœ… Phase 1 Data Exploration Complete!")
print("ðŸš€ Ready to proceed to Feature Engineering!")
print("=" * 80)
